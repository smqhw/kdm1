{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "mount_file_id": "1ucKiDdmmyhOn3sOtWhdrl2E6abRqAEYX",
      "authorship_tag": "ABX9TyP8jznPSa44xFCYfGua5DLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smqhw/kdm1/blob/main/icp5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf2rqZJRjaZG",
        "outputId": "e37b25bf-c5d5-4336-9455-ed2d8691b7f0"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVAFICInkZXU"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec\r\n",
        "\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0CHnUxukuC4"
      },
      "source": [
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtxkbBMLlRbn"
      },
      "source": [
        "# creating spark dataframe wiht the input data. You can also read the data from file. label represents the 3 documnets (0.0,0.1,0.2)\r\n",
        "sentenceData = spark.createDataFrame([\r\n",
        "       (0.0, \"Welcome to KDM TF_IDF Tutorial.\"),\r\n",
        "       (0.1, \"Learn Spark ml tf_idf in today's lab.\"),\r\n",
        "        (0.2,\"Spark Mllib has TF-IDF.\")\r\n",
        "     ], [\"label\", \"sentence\"])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE53cL5yfQCz"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMx5U9gAo44J"
      },
      "source": [
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(sentenceData)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gss7ZLw9erFH",
        "outputId": "4f445c48-2c7a-4a64-bcd3-c4cffa2ec477"
      },
      "source": [
        "wordsData.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            sentence|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|\n",
            "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|\n",
            "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlhZLikIpY7F",
        "outputId": "6abc7b46-3d86-4eee-bbe6-b96720416a9b"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\r\n",
        "featurizedData = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\"\r\n",
        "featurizedData.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|         rawFeatures|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|(20,[2,8,13,15,17...|\n",
            "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|(20,[2,3,6,7],[2....|\n",
            "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|(20,[6,14,15],[2....|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUOW38raqBYv",
        "outputId": "1580b53b-8330-4343-f365-64754de37b7d"
      },
      "source": [
        "# calculating the IDF\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idfModel = idf.fit(featurizedData)\r\n",
        "rescaledData = idfModel.transform(featurizedData)\r\n",
        "rescaledData.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|            sentence|               words|         rawFeatures|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0.0|Welcome to KDM TF...|[welcome, to, kdm...|(20,[2,8,13,15,17...|(20,[2,8,13,15,17...|\n",
            "|  0.1|Learn Spark ml tf...|[learn, spark, ml...|(20,[2,3,6,7],[2....|(20,[2,3,6,7],[0....|\n",
            "|  0.2|Spark Mllib has T...|[spark, mllib, ha...|(20,[6,14,15],[2....|(20,[6,14,15],[0....|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W8fVQxSUoUy",
        "outputId": "aaf84cda-5185-4620-c184-d2db31d7f498"
      },
      "source": [
        "#displaying the results\r\n",
        "rescaledData.select(\"label\", \"features\").show(10)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[2,8,13,15,17...|\n",
            "|  0.1|(20,[2,3,6,7],[0....|\n",
            "|  0.2|(20,[6,14,15],[0....|\n",
            "+-----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UyozfBBU7bX"
      },
      "source": [
        "spark2 = SparkSession.builder.appName(\"Ngram Example\").getOrCreate()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHol3L5QVK69",
        "outputId": "d939b24a-5255-4d4f-ccb5-16b65e4e5b4f"
      },
      "source": [
        "#creating dataframe of input\\r\\n\",\r\n",
        "wordDataFrame = spark2.createDataFrame([\r\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\r\n",
        "    (1, [\"I\", \"wish\",\"Java\", \"could\", \"use\", \"case\", \"classes\"]),\r\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\r\n",
        "], [\"id\", \"words\"])\r\n",
        "\r\n",
        "wordDataFrame.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|               words|\n",
            "+---+--------------------+\n",
            "|  0|[Hi, I, heard, ab...|\n",
            "|  1|[I, wish, Java, c...|\n",
            "|  2|[Logistic, regres...|\n",
            "+---+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD3cvpkoWFH_"
      },
      "source": [
        "#creating NGrams with n=2 (two words)\r\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)\r\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1zOXEA1e8oY",
        "outputId": "678edc92-122c-4cf0-c88f-b7690ed32902"
      },
      "source": [
        "# displaying the results\r\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|ngrams                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Hi I, I heard, heard about, about Spark]                         |\n",
            "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
            "|[Logistic regression, regression models, models are, are neat]    |\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L781MNx1fKvK"
      },
      "source": [
        "# creating spark session\r\n",
        "spark3 = SparkSession.builder.appName(\"Word2Vec Example\").getOrCreate()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqIk4nEkfZSn",
        "outputId": "4735f993-b502-46fb-c777-ebc525b380cc"
      },
      "source": [
        "# Input data: Each row is a bag of words from a sentence or document.\r\n",
        "documentDF = spark3.createDataFrame([\r\n",
        "    (\"McCarthy was asked to analyse the data from the first phase of trials of the vaccine.\".split(\" \"), ),\r\n",
        "    (\"We have amassed the raw data and are about to begin analysing it.\".split(\" \"), ),\r\n",
        "    (\"Without more data we cannot make a meaningful comparison of the two systems.\".split(\" \"), ),\r\n",
        "    (\"Collecting data is a painfully slow process.\".split(\" \"), ),\r\n",
        "    (\"You need a long series of data to be able to discern such a trend.\".split(\" \"), )\r\n",
        "], [\"text\"])\r\n",
        "documentDF.show(truncate=False)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                  |\n",
            "+------------------------------------------------------------------------------------------------------+\n",
            "|[McCarthy, was, asked, to, analyse, the, data, from, the, first, phase, of, trials, of, the, vaccine.]|\n",
            "|[We, have, amassed, the, raw, data, and, are, about, to, begin, analysing, it.]                       |\n",
            "|[Without, more, data, we, cannot, make, a, meaningful, comparison, of, the, two, systems.]            |\n",
            "|[Collecting, data, is, a, painfully, slow, process.]                                                  |\n",
            "|[You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.]                    |\n",
            "+------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rFrU5F1giT2",
        "outputId": "3bbe7c6b-4595-468d-8dbe-9415bc283638"
      },
      "source": [
        "# Learn a mapping from words to Vectors.\r\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=3, inputCol=\"text\", outputCol=\"result\")  \r\n",
        "model = word2Vec.fit(documentDF)\r\n",
        "result = model.transform(documentDF)\r\n",
        "result.show(truncate=False)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|text                                                                                                  |result                                                            |\n",
            "+------------------------------------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|[McCarthy, was, asked, to, analyse, the, data, from, the, first, phase, of, trials, of, the, vaccine.]|[0.009438527398742735,0.014125154353678226,-1.5464494936168194E-4]|\n",
            "|[We, have, amassed, the, raw, data, and, are, about, to, begin, analysing, it.]                       |[0.008283712113132844,0.01625597133086278,-0.0036636972083495217] |\n",
            "|[Without, more, data, we, cannot, make, a, meaningful, comparison, of, the, two, systems.]            |[-0.004112093064647455,0.005453866261702318,-0.005429106263013987]|\n",
            "|[Collecting, data, is, a, painfully, slow, process.]                                                  |[-0.010731614327856472,0.009080405746187481,-0.013307893382651464]|\n",
            "|[You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.]                    |[-0.009724316745996475,0.02245333989461263,-5.766247709592183E-4] |\n",
            "+------------------------------------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZUSQOtnhfbG",
        "outputId": "7e326e05-1b2d-4454-e057-dd1e438c7e2a"
      },
      "source": [
        "for row in result.collect():\r\n",
        "    text, vector = row\r\n",
        "#printing the results\r\n",
        "print(\"Text: [%s] => \\\\nVector: %s\\\\n\" % (\", \".join(text), str(vector)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: [You, need, a, long, series, of, data, to, be, able, to, discern, such, a, trend.] => \\nVector: [0.004335567106803258,0.05279908715747297,-0.02296845242381096]\\n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsJaWQ2NiXG9",
        "outputId": "5f24a504-0c4f-45ab-9914-44916b2f5be6"
      },
      "source": [
        "# showing the synonyms and cosine similarity of the word in input data\r\n",
        "synonyms = model.findSynonyms(\"data\", 10)   \r\n",
        "# its okay for certain words , real bad for others\r\n",
        "synonyms.show(10)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------------------+\n",
            "|     word|        similarity|\n",
            "+---------+------------------+\n",
            "|painfully| 0.979426383972168|\n",
            "|    asked| 0.889884352684021|\n",
            "|       be| 0.866216242313385|\n",
            "|     long|0.8165060877799988|\n",
            "|    begin| 0.814436137676239|\n",
            "|    about|0.7007120847702026|\n",
            "|       to|0.6835429668426514|\n",
            "|       of| 0.676819384098053|\n",
            "|  analyse|0.6697208881378174|\n",
            "|     more|0.6340227127075195|\n",
            "+---------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fokTIQWir-0"
      },
      "source": [
        "#closing the spark sessions\\r\\n\",\r\n",
        "spark.stop()\r\n",
        "spark2.stop()\r\n",
        "spark3.stop()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fToARBl_s_e1"
      },
      "source": [
        "with open(\"/content/text/a1.txt\",\"r+\") as a1:\r\n",
        " doc1 = a1.read()\r\n",
        "with open(\"/content/text/a2.txt\",\"r+\") as a2:\r\n",
        " doc2 = a2.read()\r\n",
        "with open(\"/content/text/a3.txt\",\"r+\") as a3:\r\n",
        " doc3 = a3.read()\r\n",
        "with open(\"/content/text/a4.txt\",\"r+\") as a4:\r\n",
        " doc4 = a4.read()\r\n",
        "with open(\"/content/text/a5.txt\",\"r+\") as a5:\r\n",
        " doc5 = a5.read()\r\n",
        "# Read all 5 txt files which contains news articles\r\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VgZgyly4BWj"
      },
      "source": [
        "# **a.Find out the top10 TF-IDF words for the above input.**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_m-fzBx4Ii8",
        "outputId": "20d070e5-fb1e-443f-fc37-e5d9465d1aa1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import pandas as pd\r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "pd.set_option('display.max_columns', 20)\r\n",
        "df.loc['Total'] = df.sum()\r\n",
        "# adding row to value total\r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           have       you     price       had        in  administration  \\\n",
            "0      0.088045  0.394401  0.000000  0.000000  0.148133        0.074066   \n",
            "1      0.000000  0.000000  0.000000  0.177663  0.074728        0.149456   \n",
            "2      0.097540  0.000000  0.000000  0.000000  0.000000        0.000000   \n",
            "3      0.199028  0.000000  0.239767  0.099514  0.083714        0.083714   \n",
            "4      0.000000  0.000000  0.218510  0.181383  0.152585        0.152585   \n",
            "Total  0.384613  0.394401  0.458277  0.458561  0.459160        0.459822   \n",
            "\n",
            "             of        to      that       the  \n",
            "0      0.088045  0.088045  0.074066  0.187934  \n",
            "1      0.000000  0.000000  0.149456  0.379227  \n",
            "2      0.195080  0.097540  0.000000  0.277602  \n",
            "3      0.000000  0.298542  0.167429  0.283220  \n",
            "4      0.181383  0.000000  0.152585  0.129056  \n",
            "Total  0.464508  0.484127  0.543536  1.257040  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSw_texZ5QmT"
      },
      "source": [
        "# **`b.Find out the top10 TF-IDF words for the lemmatized input`**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SL_xAiV5XZI",
        "outputId": "12fcddcd-32d3-43e8-f6b9-231aeed5c363"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "words1 = nltk.word_tokenize(doc1)\r\n",
        "words2 = nltk.word_tokenize(doc2)\r\n",
        "words3 = nltk.word_tokenize(doc3)\r\n",
        "words4 = nltk.word_tokenize(doc4)\r\n",
        "words5 = nltk.word_tokenize(doc5)\r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "documents = [lemmatized_document1,lemmatized_document2,lemmatized_document3,lemmatized_document4,lemmatized_document5]\r\n",
        "        \r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\\r\\n\",\r\n",
        "vect = TfidfVectorizer()\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "        \r\n",
        "df.loc['Total'] = df.sum() \r\n",
        "# adding row to value total\r\n",
        "        \r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "           have       you     price        in        of       had  \\\n",
            "0      0.088045  0.394401  0.000000  0.148133  0.088045  0.000000   \n",
            "1      0.000000  0.000000  0.000000  0.077762  0.000000  0.184877   \n",
            "2      0.097903  0.000000  0.000000  0.000000  0.195807  0.000000   \n",
            "3      0.199028  0.000000  0.239767  0.083714  0.000000  0.099514   \n",
            "4      0.000000  0.000000  0.218510  0.152585  0.181383  0.181383   \n",
            "Total  0.384977  0.394401  0.458277  0.462194  0.465235  0.465775   \n",
            "\n",
            "       administration        to      that       the  \n",
            "0            0.074066  0.088045  0.074066  0.187934  \n",
            "1            0.155525  0.000000  0.155525  0.394625  \n",
            "2            0.000000  0.097903  0.000000  0.278636  \n",
            "3            0.083714  0.298542  0.167429  0.283220  \n",
            "4            0.152585  0.000000  0.152585  0.129056  \n",
            "Total        0.465890  0.484491  0.549605  1.273471  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQtV8gJ17S_3"
      },
      "source": [
        "# **c.Find out the top10TF-IDF words for the n-gram based input.**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywEq1GBM7bIG",
        "outputId": "c9cd8dfb-9fc7-4bdc-db5a-3326c7a2ee65"
      },
      "source": [
        "# this function takes document and n  int value to generate list of n grams\r\n",
        "def ngrams(input, n):\r\n",
        "  input=input.split(' ')\r\n",
        "  output= []\r\n",
        "  for i in range(len(input)-n+1): \r\n",
        "      output.append(input[i:i+n])\r\n",
        "  return output\r\n",
        "        \r\n",
        "ngram_doc1 = ' '.join([' '.join(x) for x in ngrams(doc1, 3)])\r\n",
        "ngram_doc2 = ' '.join([' '.join(x) for x in ngrams(doc2, 3)])\r\n",
        "ngram_doc3 = ' '.join([' '.join(x) for x in ngrams(doc3, 3)])\r\n",
        "ngram_doc4 = ' '.join([' '.join(x) for x in ngrams(doc4, 3)])\r\n",
        "ngram_doc5 = ' '.join([' '.join(x) for x in ngrams(doc5, 3)])\r\n",
        "        \r\n",
        "# documents = [ngram_doc1,ngram_doc2,ngram_doc3,ngram_doc4,ngram_doc5]\r\n",
        "  \r\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]\r\n",
        "        \r\n",
        "# using sklearn library which has inbuilt Tfidf vectorizer class which can generate tfidf for given corpus\r\n",
        "vect = TfidfVectorizer( ngram_range=(3,3)) \r\n",
        "# TfidfVectorizer has inbuilt ngram kwarg which show tfidf for ngrams\r\n",
        "#created TfidfVectorizer object\r\n",
        "tfidf_matrix = vect.fit_transform(documents)\r\n",
        "#passed list of documents or corpus to obt method fit_transform\r\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\r\n",
        "# converted method output to panda data frame \r\n",
        "        \r\n",
        "df.loc['Total'] = df.sum() \r\n",
        "# adding row to value total\r\n",
        "        \r\n",
        "#filtering values of words whos tfidf is greater than 0.3\r\n",
        "# also used transpose function here to filter out words (which was rows) and then converted matrix back to original version\r\n",
        "print (df.T.sort_values('Total', ascending=True).tail(10).T)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       under the trump  reporter matt lee  that all of  lee piped in  \\\n",
            "0             0.000000           0.000000     0.000000      0.000000   \n",
            "1             0.000000           0.000000     0.000000      0.000000   \n",
            "2             0.000000           0.000000     0.000000      0.000000   \n",
            "3             0.000000           0.000000     0.000000      0.000000   \n",
            "4             0.238022           0.238022     0.238022      0.238022   \n",
            "Total         0.238022           0.238022     0.238022      0.238022   \n",
            "\n",
            "       piped in telling  price that all  this work had  press reporter matt  \\\n",
            "0              0.000000        0.000000       0.000000             0.000000   \n",
            "1              0.000000        0.000000       0.000000             0.000000   \n",
            "2              0.000000        0.000000       0.000000             0.000000   \n",
            "3              0.000000        0.000000       0.000000             0.000000   \n",
            "4              0.238022        0.238022       0.238022             0.238022   \n",
            "Total          0.238022        0.238022       0.238022             0.238022   \n",
            "\n",
            "       begun under the  the trump administration  \n",
            "0             0.000000                  0.000000  \n",
            "1             0.000000                  0.108150  \n",
            "2             0.000000                  0.000000  \n",
            "3             0.000000                  0.000000  \n",
            "4             0.238022                  0.192034  \n",
            "Total         0.238022                  0.300184  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Was1FG86-bDZ"
      },
      "source": [
        "# **2.Write a simple spark program to read a dataset and find the W2V similar words (words with higher cosine similarity) for the Top10 TF-IDF Words**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCazQCpH-me3",
        "outputId": "2c79d9f9-e63e-41cc-8b3c-62c658a05284"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml.feature import NGram\r\n",
        "from pyspark.ml.feature import Word2Vec\r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "        \r\n",
        "documentData = spark.createDataFrame([\r\n",
        "                 (0.0, doc1),\r\n",
        "                 (0.1, doc2),\r\n",
        "                 (0.2, doc3),\r\n",
        "                 (0.3, doc4),\r\n",
        "                (0.5, doc5)\r\n",
        "            ], [\"label\", \"document\"])\r\n",
        "        \r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|\"You guys have on...|[\"you, guys, have...|\n",
            "|  0.1|Senior Republican...|[senior, republic...|\n",
            "|  0.2|Kremlin critics h...|[kremlin, critics...|\n",
            "|  0.3|At Monday’s press...|[at, monday’s, pr...|\n",
            "|  0.5|Associated Press ...|[associated, pres...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2EtsQtS_uQi",
        "outputId": "bd84b58e-60e7-4516-fe6a-ba4c407d0a06"
      },
      "source": [
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(wordsData)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "print(\"TF-IDF without NLP:\")\r\n",
        "for each in tfidf.collect():\r\n",
        " print(each)\r\n",
        "print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[5,6,8,16,17...|\n",
            "|  0.1|(200,[5,8,16,17,2...|\n",
            "|  0.2|(200,[7,11,16,17,...|\n",
            "|  0.3|(200,[3,6,8,16,17...|\n",
            "|  0.5|(200,[17,36,56,65...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF without NLP:\n",
            "Row(label=0.0, document='\"You guys have only been in office for a month, right? Are you telling me that in the last four weeks these 18 companies all of the sudden decided to say, ‘Oh my God! We better not doing anything with Nord Stream 2,\" Lee said. \"You guys are taking credit for stuff the previous administration did. Yes or no?\"', words=['\"you', 'guys', 'have', 'only', 'been', 'in', 'office', 'for', 'a', 'month,', 'right?', 'are', 'you', 'telling', 'me', 'that', 'in', 'the', 'last', 'four', 'weeks', 'these', '18', 'companies', 'all', 'of', 'the', 'sudden', 'decided', 'to', 'say,', '‘oh', 'my', 'god!', 'we', 'better', 'not', 'doing', 'anything', 'with', 'nord', 'stream', '2,\"', 'lee', 'said.', '\"you', 'guys', 'are', 'taking', 'credit', 'for', 'stuff', 'the', 'previous', 'administration', 'did.', 'yes', 'or', 'no?\"'], rawFeatures=SparseVector(200, {5: 1.0, 6: 1.0, 8: 1.0, 16: 2.0, 17: 3.0, 21: 1.0, 29: 1.0, 43: 1.0, 44: 1.0, 47: 1.0, 50: 1.0, 56: 1.0, 63: 3.0, 67: 3.0, 70: 1.0, 88: 1.0, 95: 1.0, 96: 1.0, 98: 2.0, 99: 2.0, 112: 1.0, 114: 1.0, 117: 1.0, 122: 1.0, 123: 1.0, 126: 1.0, 128: 1.0, 135: 1.0, 138: 1.0, 141: 1.0, 143: 1.0, 144: 3.0, 145: 2.0, 148: 2.0, 151: 1.0, 160: 1.0, 163: 2.0, 165: 1.0, 168: 1.0, 169: 1.0, 172: 1.0, 185: 1.0, 188: 1.0, 189: 1.0, 196: 1.0}), features=SparseVector(200, {5: 0.6931, 6: 0.6931, 8: 0.4055, 16: 0.3646, 17: 0.0, 21: 0.6931, 29: 0.6931, 43: 1.0986, 44: 1.0986, 47: 1.0986, 50: 0.6931, 56: 0.6931, 63: 1.2164, 67: 1.2164, 70: 0.4055, 88: 0.4055, 95: 0.4055, 96: 0.4055, 98: 1.3863, 99: 0.8109, 112: 0.1823, 114: 1.0986, 117: 0.6931, 122: 1.0986, 123: 0.6931, 126: 1.0986, 128: 1.0986, 135: 0.6931, 138: 0.4055, 141: 0.6931, 143: 1.0986, 144: 0.547, 145: 0.8109, 148: 2.1972, 151: 1.0986, 160: 0.1823, 163: 2.1972, 165: 0.6931, 168: 0.6931, 169: 0.1823, 172: 0.6931, 185: 0.6931, 188: 0.6931, 189: 1.0986, 196: 1.0986}))\n",
            "Row(label=0.1, document='Senior Republican lawmakers criticized the move as inadequate, noting that the administration had not penalized any additional companies or individuals for work on the Nord Stream 2 pipeline. They also said the new sanctions were redundant as they duplicate existing penalties that the Trump administration had imposed on the pipelaying ship Fortuna and its owner KVT-RUS in January.', words=['senior', 'republican', 'lawmakers', 'criticized', 'the', 'move', 'as', 'inadequate,', 'noting', 'that', 'the', 'administration', 'had', 'not', 'penalized', 'any', 'additional', 'companies', 'or', 'individuals', 'for', 'work', 'on', 'the', 'nord', 'stream', '2', 'pipeline.', 'they', 'also', 'said', 'the', 'new', 'sanctions', 'were', 'redundant', 'as', 'they', 'duplicate', 'existing', 'penalties', 'that', 'the', 'trump', 'administration', 'had', 'imposed', 'on', 'the', 'pipelaying', 'ship', 'fortuna', 'and', 'its', 'owner', 'kvt-rus', 'in', 'january.'], rawFeatures=SparseVector(200, {5: 1.0, 8: 1.0, 16: 1.0, 17: 7.0, 21: 1.0, 22: 1.0, 24: 1.0, 27: 1.0, 46: 1.0, 48: 2.0, 49: 1.0, 53: 1.0, 58: 1.0, 62: 1.0, 63: 1.0, 67: 1.0, 69: 1.0, 70: 1.0, 82: 1.0, 91: 1.0, 92: 1.0, 98: 1.0, 107: 1.0, 111: 1.0, 112: 2.0, 119: 1.0, 120: 1.0, 127: 1.0, 136: 2.0, 139: 2.0, 141: 1.0, 144: 2.0, 145: 1.0, 149: 2.0, 160: 2.0, 162: 1.0, 169: 2.0, 171: 1.0, 185: 3.0, 192: 2.0, 199: 1.0}), features=SparseVector(200, {5: 0.6931, 8: 0.4055, 16: 0.1823, 17: 0.0, 21: 0.6931, 22: 0.6931, 24: 0.6931, 27: 1.0986, 46: 1.0986, 48: 2.1972, 49: 0.6931, 53: 1.0986, 58: 1.0986, 62: 1.0986, 63: 0.4055, 67: 0.4055, 69: 1.0986, 70: 0.4055, 82: 1.0986, 91: 0.6931, 92: 1.0986, 98: 0.6931, 107: 0.6931, 111: 0.6931, 112: 0.3646, 119: 1.0986, 120: 0.6931, 127: 0.6931, 136: 0.8109, 139: 2.1972, 141: 0.6931, 144: 0.3646, 145: 0.4055, 149: 2.1972, 160: 0.3646, 162: 0.6931, 169: 0.3646, 171: 0.6931, 185: 2.0794, 192: 2.1972, 199: 0.6931}))\n",
            "Row(label=0.2, document='Kremlin critics have said the project would double the amount of natural gas imported from Russia. If completed, it would leave Europe more dependent on Russian energy than ever before. The poisoning of Russian dissident Alexei Navalny has revitalized calls for the project to be abandoned.', words=['kremlin', 'critics', 'have', 'said', 'the', 'project', 'would', 'double', 'the', 'amount', 'of', 'natural', 'gas', 'imported', 'from', 'russia.', 'if', 'completed,', 'it', 'would', 'leave', 'europe', 'more', 'dependent', 'on', 'russian', 'energy', 'than', 'ever', 'before.', 'the', 'poisoning', 'of', 'russian', 'dissident', 'alexei', 'navalny', 'has', 'revitalized', 'calls', 'for', 'the', 'project', 'to', 'be', 'abandoned.'], rawFeatures=SparseVector(200, {7: 1.0, 11: 1.0, 16: 1.0, 17: 4.0, 22: 1.0, 24: 1.0, 29: 1.0, 30: 2.0, 49: 1.0, 55: 1.0, 59: 1.0, 61: 1.0, 66: 1.0, 76: 1.0, 86: 1.0, 88: 1.0, 95: 2.0, 96: 1.0, 99: 1.0, 102: 1.0, 107: 2.0, 111: 3.0, 113: 3.0, 117: 1.0, 120: 1.0, 121: 1.0, 123: 3.0, 136: 1.0, 144: 1.0, 161: 1.0, 188: 1.0, 194: 1.0, 197: 1.0, 198: 1.0}), features=SparseVector(200, {7: 1.0986, 11: 1.0986, 16: 0.1823, 17: 0.0, 22: 0.6931, 24: 0.6931, 29: 0.6931, 30: 2.1972, 49: 0.6931, 55: 1.0986, 59: 1.0986, 61: 1.0986, 66: 1.0986, 76: 1.0986, 86: 1.0986, 88: 0.4055, 95: 0.8109, 96: 0.4055, 99: 0.4055, 102: 1.0986, 107: 1.3863, 111: 2.0794, 113: 3.2958, 117: 0.6931, 120: 0.6931, 121: 1.0986, 123: 2.0794, 136: 0.4055, 144: 0.1823, 161: 1.0986, 188: 0.6931, 194: 1.0986, 197: 1.0986, 198: 0.4055}))\n",
            "Row(label=0.3, document='At Monday’s press conference, Price seemed to suggest that the Biden administration’s efforts had led to the development on Nord Stream 2. The aforementioned entities have wound down their activities in the pipeline project, Price said, and demonstrated that Biden and Congress’s strategy have been \"working to good effect.\"', words=['at', 'monday’s', 'press', 'conference,', 'price', 'seemed', 'to', 'suggest', 'that', 'the', 'biden', 'administration’s', 'efforts', 'had', 'led', 'to', 'the', 'development', 'on', 'nord', 'stream', '2.', 'the', 'aforementioned', 'entities', 'have', 'wound', 'down', 'their', 'activities', 'in', 'the', 'pipeline', 'project,', 'price', 'said,', 'and', 'demonstrated', 'that', 'biden', 'and', 'congress’s', 'strategy', 'have', 'been', '\"working', 'to', 'good', 'effect.\"'], rawFeatures=SparseVector(200, {3: 2.0, 6: 1.0, 8: 2.0, 16: 1.0, 17: 5.0, 36: 1.0, 40: 1.0, 50: 1.0, 60: 1.0, 63: 1.0, 67: 1.0, 68: 1.0, 70: 1.0, 71: 1.0, 88: 3.0, 91: 3.0, 99: 2.0, 112: 1.0, 135: 3.0, 136: 1.0, 138: 2.0, 144: 1.0, 156: 1.0, 160: 3.0, 162: 1.0, 165: 1.0, 167: 1.0, 168: 1.0, 169: 1.0, 175: 1.0, 182: 1.0, 198: 1.0, 199: 1.0}), features=SparseVector(200, {3: 2.1972, 6: 0.6931, 8: 0.8109, 16: 0.1823, 17: 0.0, 36: 0.6931, 40: 1.0986, 50: 0.6931, 60: 1.0986, 63: 0.4055, 67: 0.4055, 68: 1.0986, 70: 0.4055, 71: 1.0986, 88: 1.2164, 91: 2.0794, 99: 0.8109, 112: 0.1823, 135: 2.0794, 136: 0.4055, 138: 0.8109, 144: 0.1823, 156: 1.0986, 160: 0.547, 162: 0.6931, 165: 0.6931, 167: 1.0986, 168: 0.6931, 169: 0.1823, 175: 1.0986, 182: 1.0986, 198: 0.4055, 199: 0.6931}))\n",
            "Row(label=0.5, document='Associated Press reporter Matt Lee piped in, telling Price that all of this work had begun under the Trump administration.', words=['associated', 'press', 'reporter', 'matt', 'lee', 'piped', 'in,', 'telling', 'price', 'that', 'all', 'of', 'this', 'work', 'had', 'begun', 'under', 'the', 'trump', 'administration.'], rawFeatures=SparseVector(200, {17: 1.0, 36: 1.0, 56: 1.0, 65: 1.0, 84: 1.0, 95: 1.0, 96: 1.0, 112: 1.0, 127: 1.0, 130: 1.0, 134: 1.0, 138: 1.0, 145: 1.0, 160: 1.0, 169: 1.0, 171: 1.0, 172: 1.0, 173: 1.0, 181: 1.0, 198: 1.0}), features=SparseVector(200, {17: 0.0, 36: 0.6931, 56: 0.6931, 65: 1.0986, 84: 1.0986, 95: 0.4055, 96: 0.4055, 112: 0.1823, 127: 0.6931, 130: 1.0986, 134: 1.0986, 138: 0.4055, 145: 0.4055, 160: 0.1823, 169: 0.1823, 171: 0.6931, 172: 0.6931, 173: 1.0986, 181: 1.0986, 198: 0.4055}))\n",
            "(200,[17,36,56,65,84,95,96,112,127,130,134,138,145,160,169,171,172,173,181,198],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96b3XmRyBfR-"
      },
      "source": [
        "# **b.Try with Lemmatization**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjv71I3hC6jV",
        "outputId": "352659c2-afb8-4097-bf7a-643222ceec71"
      },
      "source": [
        "import nltk;nltk.download('punkt');nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "words1 = nltk.word_tokenize(doc1)\r\n",
        "words2 = nltk.word_tokenize(doc2)\r\n",
        "words3 = nltk.word_tokenize(doc3)\r\n",
        "words4 = nltk.word_tokenize(doc4)\r\n",
        "words5 = nltk.word_tokenize(doc5)\r\n",
        "        \r\n",
        "lemmatized_document1 = ' '.join([lemmatizer.lemmatize(w) for w in words1])\r\n",
        "lemmatized_document2 = ' '.join([lemmatizer.lemmatize(w) for w in words2])\r\n",
        "lemmatized_document3 = ' '.join([lemmatizer.lemmatize(w) for w in words3])\r\n",
        "lemmatized_document4 = ' '.join([lemmatizer.lemmatize(w) for w in words4])\r\n",
        "lemmatized_document5 = ' '.join([lemmatizer.lemmatize(w) for w in words5])\r\n",
        "\r\n",
        "### lemmatizing words from 5 input docs same as previos task\r\n",
        "        \r\n",
        "# creating spark session\r\n",
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "        \r\n",
        "documentData = spark.createDataFrame([\r\n",
        "              (0.0, lemmatized_document1),\r\n",
        "              (0.1, lemmatized_document2),\r\n",
        "              (0.2, lemmatized_document3),\r\n",
        "              (0.3, lemmatized_document4),\r\n",
        "              (0.5, lemmatized_document5)\r\n",
        "          ], [\"label\", \"document\"])\r\n",
        "        \r\n",
        "# creating tokens/words from the sentence data\r\n",
        "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\r\n",
        "wordsData = tokenizer.transform(documentData)\r\n",
        "print (documentData)\r\n",
        "wordsData.show()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "DataFrame[label: double, document: string]\n",
            "+-----+--------------------+--------------------+\n",
            "|label|            document|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|`` You guy have o...|[``, you, guy, ha...|\n",
            "|  0.1|Senior Republican...|[senior, republic...|\n",
            "|  0.2|Kremlin critic ha...|[kremlin, critic,...|\n",
            "|  0.3|At Monday ’ s pre...|[at, monday, ’, s...|\n",
            "|  0.5|Associated Press ...|[associated, pres...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtFZvZd7FH5o",
        "outputId": "31fbbd5c-b7e6-43b6-fd3a-24bfc6754189"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"TfIdf Example\").getOrCreate()\r\n",
        "documentData = spark.createDataFrame([\r\n",
        "         (0.0, doc1.split(' ')),\r\n",
        "         (0.1, doc2.split(' ')),\r\n",
        "         (0.2, doc3.split(' ')),\r\n",
        "         (0.3, doc4.split(' ')),\r\n",
        "         (0.4, doc5.split(' '))\r\n",
        "          ], [\"label\", \"document\"])\r\n",
        "        \r\n",
        "        \r\n",
        "ngram = NGram(n=2, inputCol=\"document\", outputCol=\"ngrams\")\r\n",
        "ngramDataFrame = ngram.transform(documentData)\r\n",
        "# applying tf on the words data\r\n",
        "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=200)\r\n",
        "tf = hashingTF.transform(ngramDataFrame)\r\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\r\n",
        "# calculating the IDF\r\n",
        "tf.cache()\r\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\r\n",
        "idf = idf.fit(tf)\r\n",
        "tfidf = idf.transform(tf)\r\n",
        "#displaying the results\r\n",
        "tfidf.select(\"label\", \"features\").show()\r\n",
        "        \r\n",
        "        \r\n",
        "print(\"TF-IDF with ngram:\")\r\n",
        "for each in tfidf.collect():\r\n",
        " print(each)\r\n",
        "print(each['rawFeatures'])\r\n",
        "spark.stop()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(200,[2,3,7,17,24...|\n",
            "|  0.1|(200,[0,1,2,3,4,8...|\n",
            "|  0.2|(200,[1,11,13,15,...|\n",
            "|  0.3|(200,[1,2,7,8,14,...|\n",
            "|  0.4|(200,[18,26,30,42...|\n",
            "+-----+--------------------+\n",
            "\n",
            "TF-IDF with ngram:\n",
            "Row(label=0.0, document=['\"You', 'guys', 'have', 'only', 'been', 'in', 'office', 'for', 'a', 'month,', 'right?', 'Are', 'you', 'telling', 'me', 'that', 'in', 'the', 'last', 'four', 'weeks', 'these', '18', 'companies', 'all', 'of', 'the', 'sudden', 'decided', 'to', 'say,', '‘Oh', 'my', 'God!', 'We', 'better', 'not', 'doing', 'anything', 'with', 'Nord', 'Stream', '2,\"', 'Lee', 'said.', '\"You', 'guys', 'are', 'taking', 'credit', 'for', 'stuff', 'the', 'previous', 'administration', 'did.', 'Yes', 'or', 'no?\"'], ngrams=['\"You guys', 'guys have', 'have only', 'only been', 'been in', 'in office', 'office for', 'for a', 'a month,', 'month, right?', 'right? Are', 'Are you', 'you telling', 'telling me', 'me that', 'that in', 'in the', 'the last', 'last four', 'four weeks', 'weeks these', 'these 18', '18 companies', 'companies all', 'all of', 'of the', 'the sudden', 'sudden decided', 'decided to', 'to say,', 'say, ‘Oh', '‘Oh my', 'my God!', 'God! We', 'We better', 'better not', 'not doing', 'doing anything', 'anything with', 'with Nord', 'Nord Stream', 'Stream 2,\"', '2,\" Lee', 'Lee said.', 'said. \"You', '\"You guys', 'guys are', 'are taking', 'taking credit', 'credit for', 'for stuff', 'stuff the', 'the previous', 'previous administration', 'administration did.', 'did. Yes', 'Yes or', 'or no?\"'], rawFeatures=SparseVector(200, {2: 1.0, 3: 1.0, 7: 1.0, 17: 1.0, 24: 1.0, 29: 2.0, 37: 2.0, 42: 2.0, 43: 1.0, 44: 1.0, 45: 1.0, 46: 1.0, 51: 1.0, 52: 1.0, 56: 1.0, 58: 1.0, 60: 1.0, 66: 1.0, 71: 2.0, 72: 1.0, 82: 1.0, 98: 1.0, 101: 1.0, 104: 1.0, 106: 2.0, 107: 1.0, 108: 1.0, 109: 1.0, 111: 1.0, 112: 1.0, 113: 1.0, 114: 1.0, 116: 1.0, 119: 1.0, 128: 1.0, 134: 1.0, 135: 1.0, 138: 1.0, 141: 1.0, 143: 1.0, 145: 1.0, 147: 1.0, 148: 1.0, 149: 1.0, 150: 1.0, 151: 1.0, 163: 1.0, 171: 2.0, 189: 2.0, 192: 1.0, 195: 1.0}), features=SparseVector(200, {2: 0.4055, 3: 0.6931, 7: 0.6931, 17: 1.0986, 24: 1.0986, 29: 2.1972, 37: 2.1972, 42: 1.3863, 43: 1.0986, 44: 0.6931, 45: 0.6931, 46: 1.0986, 51: 0.6931, 52: 0.4055, 56: 1.0986, 58: 1.0986, 60: 0.6931, 66: 1.0986, 71: 1.3863, 72: 1.0986, 82: 0.6931, 98: 0.6931, 101: 0.4055, 104: 1.0986, 106: 2.1972, 107: 1.0986, 108: 0.4055, 109: 0.6931, 111: 0.4055, 112: 0.4055, 113: 1.0986, 114: 0.6931, 116: 0.6931, 119: 0.4055, 128: 0.6931, 134: 0.6931, 135: 0.4055, 138: 0.6931, 141: 0.6931, 143: 1.0986, 145: 1.0986, 147: 0.6931, 148: 0.4055, 149: 0.6931, 150: 1.0986, 151: 0.6931, 163: 0.6931, 171: 2.1972, 189: 0.8109, 192: 1.0986, 195: 0.4055}))\n",
            "Row(label=0.1, document=['Senior', 'Republican', 'lawmakers', 'criticized', 'the', 'move', 'as', 'inadequate,', 'noting', 'that', 'the', 'administration', 'had', 'not', 'penalized', 'any', 'additional', 'companies', 'or', 'individuals', 'for', 'work', 'on', 'the', 'Nord', 'Stream', '2', 'pipeline.', 'They', 'also', 'said', 'the', 'new', 'sanctions', 'were', 'redundant', 'as', 'they', 'duplicate', 'existing', 'penalties', 'that', 'the', 'Trump', 'administration', 'had', 'imposed', 'on', 'the', 'pipelaying', 'ship', 'Fortuna', 'and', 'its', 'owner', 'KVT-RUS', 'in', 'January.'], ngrams=['Senior Republican', 'Republican lawmakers', 'lawmakers criticized', 'criticized the', 'the move', 'move as', 'as inadequate,', 'inadequate, noting', 'noting that', 'that the', 'the administration', 'administration had', 'had not', 'not penalized', 'penalized any', 'any additional', 'additional companies', 'companies or', 'or individuals', 'individuals for', 'for work', 'work on', 'on the', 'the Nord', 'Nord Stream', 'Stream 2', '2 pipeline.', 'pipeline. They', 'They also', 'also said', 'said the', 'the new', 'new sanctions', 'sanctions were', 'were redundant', 'redundant as', 'as they', 'they duplicate', 'duplicate existing', 'existing penalties', 'penalties that', 'that the', 'the Trump', 'Trump administration', 'administration had', 'had imposed', 'imposed on', 'on the', 'the pipelaying', 'pipelaying ship', 'ship Fortuna', 'Fortuna and', 'and its', 'its owner', 'owner KVT-RUS', 'KVT-RUS in', 'in January.'], rawFeatures=SparseVector(200, {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 2.0, 8: 2.0, 18: 2.0, 23: 1.0, 26: 1.0, 28: 1.0, 31: 1.0, 36: 1.0, 40: 1.0, 45: 1.0, 47: 1.0, 48: 1.0, 51: 1.0, 52: 1.0, 54: 1.0, 62: 1.0, 68: 1.0, 71: 1.0, 78: 1.0, 79: 1.0, 83: 1.0, 84: 1.0, 90: 1.0, 93: 2.0, 100: 1.0, 101: 1.0, 108: 1.0, 112: 1.0, 117: 1.0, 119: 1.0, 122: 1.0, 123: 1.0, 126: 1.0, 135: 1.0, 138: 1.0, 141: 1.0, 148: 1.0, 149: 2.0, 152: 1.0, 155: 1.0, 168: 1.0, 173: 3.0, 181: 1.0, 185: 1.0, 195: 1.0, 199: 1.0}), features=SparseVector(200, {0: 1.0986, 1: 0.4055, 2: 0.4055, 3: 0.6931, 4: 2.1972, 8: 1.3863, 18: 1.3863, 23: 0.4055, 26: 0.6931, 28: 1.0986, 31: 1.0986, 36: 0.6931, 40: 1.0986, 45: 0.6931, 47: 1.0986, 48: 1.0986, 51: 0.6931, 52: 0.4055, 54: 0.6931, 62: 1.0986, 68: 1.0986, 71: 0.6931, 78: 1.0986, 79: 1.0986, 83: 0.6931, 84: 0.4055, 90: 0.4055, 93: 1.3863, 100: 1.0986, 101: 0.4055, 108: 0.4055, 112: 0.4055, 117: 1.0986, 119: 0.4055, 122: 0.6931, 123: 1.0986, 126: 1.0986, 135: 0.4055, 138: 0.6931, 141: 0.6931, 148: 0.4055, 149: 1.3863, 152: 1.0986, 155: 0.6931, 168: 1.0986, 173: 2.0794, 181: 1.0986, 185: 0.6931, 195: 0.4055, 199: 1.0986}))\n",
            "Row(label=0.2, document=['Kremlin', 'critics', 'have', 'said', 'the', 'project', 'would', 'double', 'the', 'amount', 'of', 'natural', 'gas', 'imported', 'from', 'Russia.', 'If', 'completed,', 'it', 'would', 'leave', 'Europe', 'more', 'dependent', 'on', 'Russian', 'energy', 'than', 'ever', 'before.', 'The', 'poisoning', 'of', 'Russian', 'dissident', 'Alexei', 'Navalny', 'has', 'revitalized', 'calls', 'for', 'the', 'project', 'to', 'be', 'abandoned.'], ngrams=['Kremlin critics', 'critics have', 'have said', 'said the', 'the project', 'project would', 'would double', 'double the', 'the amount', 'amount of', 'of natural', 'natural gas', 'gas imported', 'imported from', 'from Russia.', 'Russia. If', 'If completed,', 'completed, it', 'it would', 'would leave', 'leave Europe', 'Europe more', 'more dependent', 'dependent on', 'on Russian', 'Russian energy', 'energy than', 'than ever', 'ever before.', 'before. The', 'The poisoning', 'poisoning of', 'of Russian', 'Russian dissident', 'dissident Alexei', 'Alexei Navalny', 'Navalny has', 'has revitalized', 'revitalized calls', 'calls for', 'for the', 'the project', 'project to', 'to be', 'be abandoned.'], rawFeatures=SparseVector(200, {1: 1.0, 11: 1.0, 13: 1.0, 15: 1.0, 20: 1.0, 21: 1.0, 23: 1.0, 30: 2.0, 36: 1.0, 38: 1.0, 57: 1.0, 73: 1.0, 80: 1.0, 83: 1.0, 84: 1.0, 85: 1.0, 90: 1.0, 95: 2.0, 98: 1.0, 108: 1.0, 109: 1.0, 111: 1.0, 112: 1.0, 128: 1.0, 130: 1.0, 134: 1.0, 140: 2.0, 147: 1.0, 151: 1.0, 155: 1.0, 161: 1.0, 162: 1.0, 167: 1.0, 170: 1.0, 185: 2.0, 186: 1.0, 189: 1.0, 193: 1.0, 194: 2.0, 198: 1.0}), features=SparseVector(200, {1: 0.4055, 11: 1.0986, 13: 1.0986, 15: 1.0986, 20: 1.0986, 21: 1.0986, 23: 0.4055, 30: 1.3863, 36: 0.6931, 38: 1.0986, 57: 1.0986, 73: 1.0986, 80: 1.0986, 83: 0.6931, 84: 0.4055, 85: 0.6931, 90: 0.4055, 95: 1.3863, 98: 0.6931, 108: 0.4055, 109: 0.6931, 111: 0.4055, 112: 0.4055, 128: 0.6931, 130: 1.0986, 134: 0.6931, 140: 2.1972, 147: 0.6931, 151: 0.6931, 155: 0.6931, 161: 0.6931, 162: 1.0986, 167: 0.6931, 170: 1.0986, 185: 1.3863, 186: 1.0986, 189: 0.4055, 193: 1.0986, 194: 2.1972, 198: 1.0986}))\n",
            "Row(label=0.3, document=['At', 'Monday’s', 'press', 'conference,', 'Price', 'seemed', 'to', 'suggest', 'that', 'the', 'Biden', 'administration’s', 'efforts', 'had', 'led', 'to', 'the', 'development', 'on', 'Nord', 'Stream', '2.', 'The', 'aforementioned', 'entities', 'have', 'wound', 'down', 'their', 'activities', 'in', 'the', 'pipeline', 'project,', 'Price', 'said,', 'and', 'demonstrated', 'that', 'Biden', 'and', 'Congress’s', 'strategy', 'have', 'been', '\"working', 'to', 'good', 'effect.\"'], ngrams=['At Monday’s', 'Monday’s press', 'press conference,', 'conference, Price', 'Price seemed', 'seemed to', 'to suggest', 'suggest that', 'that the', 'the Biden', 'Biden administration’s', 'administration’s efforts', 'efforts had', 'had led', 'led to', 'to the', 'the development', 'development on', 'on Nord', 'Nord Stream', 'Stream 2.', '2. The', 'The aforementioned', 'aforementioned entities', 'entities have', 'have wound', 'wound down', 'down their', 'their activities', 'activities in', 'in the', 'the pipeline', 'pipeline project,', 'project, Price', 'Price said,', 'said, and', 'and demonstrated', 'demonstrated that', 'that Biden', 'Biden and', 'and Congress’s', 'Congress’s strategy', 'strategy have', 'have been', 'been \"working', '\"working to', 'to good', 'good effect.\"'], rawFeatures=SparseVector(200, {1: 1.0, 2: 1.0, 7: 1.0, 8: 1.0, 14: 1.0, 23: 2.0, 27: 1.0, 44: 2.0, 50: 1.0, 52: 1.0, 54: 1.0, 55: 1.0, 61: 1.0, 82: 1.0, 84: 1.0, 85: 1.0, 87: 1.0, 90: 1.0, 91: 1.0, 92: 1.0, 96: 1.0, 97: 1.0, 102: 1.0, 111: 2.0, 114: 1.0, 116: 1.0, 118: 1.0, 119: 1.0, 122: 2.0, 125: 1.0, 129: 1.0, 132: 1.0, 135: 1.0, 144: 1.0, 148: 1.0, 161: 1.0, 167: 1.0, 169: 1.0, 172: 1.0, 173: 1.0, 187: 2.0, 188: 1.0, 189: 1.0}), features=SparseVector(200, {1: 0.4055, 2: 0.4055, 7: 0.6931, 8: 0.6931, 14: 1.0986, 23: 0.8109, 27: 1.0986, 44: 1.3863, 50: 0.6931, 52: 0.4055, 54: 0.6931, 55: 1.0986, 61: 1.0986, 82: 0.6931, 84: 0.4055, 85: 0.6931, 87: 0.6931, 90: 0.4055, 91: 0.6931, 92: 1.0986, 96: 1.0986, 97: 1.0986, 102: 1.0986, 111: 0.8109, 114: 0.6931, 116: 0.6931, 118: 1.0986, 119: 0.4055, 122: 1.3863, 125: 1.0986, 129: 1.0986, 132: 1.0986, 135: 0.4055, 144: 1.0986, 148: 0.4055, 161: 0.6931, 167: 0.6931, 169: 1.0986, 172: 0.6931, 173: 0.6931, 187: 2.1972, 188: 0.6931, 189: 0.4055}))\n",
            "Row(label=0.4, document=['Associated', 'Press', 'reporter', 'Matt', 'Lee', 'piped', 'in,', 'telling', 'Price', 'that', 'all', 'of', 'this', 'work', 'had', 'begun', 'under', 'the', 'Trump', 'administration.'], ngrams=['Associated Press', 'Press reporter', 'reporter Matt', 'Matt Lee', 'Lee piped', 'piped in,', 'in, telling', 'telling Price', 'Price that', 'that all', 'all of', 'of this', 'this work', 'work had', 'had begun', 'begun under', 'under the', 'the Trump', 'Trump administration.'], rawFeatures=SparseVector(200, {18: 1.0, 26: 1.0, 30: 1.0, 42: 1.0, 50: 1.0, 60: 1.0, 87: 1.0, 88: 1.0, 91: 1.0, 93: 1.0, 95: 1.0, 101: 1.0, 156: 1.0, 159: 1.0, 163: 1.0, 172: 1.0, 179: 1.0, 188: 1.0, 195: 1.0}), features=SparseVector(200, {18: 0.6931, 26: 0.6931, 30: 0.6931, 42: 0.6931, 50: 0.6931, 60: 0.6931, 87: 0.6931, 88: 1.0986, 91: 0.6931, 93: 0.6931, 95: 0.6931, 101: 0.4055, 156: 1.0986, 159: 1.0986, 163: 0.6931, 172: 0.6931, 179: 1.0986, 188: 0.6931, 195: 0.4055}))\n",
            "(200,[18,26,30,42,50,60,87,88,91,93,95,101,156,159,163,172,179,188,195],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLwmHWRRFVDP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}